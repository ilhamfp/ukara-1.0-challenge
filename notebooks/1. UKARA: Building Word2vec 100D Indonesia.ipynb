{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKARA: Building Word2Vec 100 Indonesia\n",
    "\n",
    "This notebook produced the word2vec word embedding I use in my Ukara NLP Challenge submission. For more information, check the repository. \n",
    "\n",
    "\n",
    "Repository: [https://github.com/ilhamfp/ukara-1.0-challenge](https://github.com/ilhamfp/ukara-1.0-challenge)\n",
    "\n",
    "This notebook was originally ran on a kaggle kernel. Check out the [kernel](https://www.kaggle.com/ilhamfp31/ukara-building-word2vec-100-indonesia) and the [output dataset](https://www.kaggle.com/ilhamfp31/word2vec-100-indonesian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Importing libraries and setting contant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version:  3.8.1\n",
      "pandas version:  0.25.1\n",
      "requests version:  2.22.0\n",
      "re version:  2.2.1\n",
      "argparse version:  1.1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"gensim version: \", gensim.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas version: \", pd.__version__)\n",
    "\n",
    "import requests\n",
    "print(\"requests version: \", requests.__version__)\n",
    "\n",
    "import re\n",
    "print(\"re version: \", re.__version__)\n",
    "\n",
    "import argparse\n",
    "print(\"argparse version: \", argparse.__version__)\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "import multiprocessing\n",
    "\n",
    "DIR_DATA_A = \"../data/data_A\"\n",
    "DIR_DATA_B = \"../data/data_B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Ukara Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A_train = pd.read_csv(\"{}/data_train_A.csv\".format(DIR_DATA_A))\n",
    "data_A_dev = pd.read_csv(\"{}/data_dev_A.csv\".format(DIR_DATA_A))\n",
    "data_A_test = pd.read_csv(\"{}/data_test_A.csv\".format(DIR_DATA_A))\n",
    "\n",
    "data_B_train = pd.read_csv(\"{}/data_train_B.csv\".format(DIR_DATA_B))\n",
    "data_B_dev = pd.read_csv(\"{}/data_dev_B.csv\".format(DIR_DATA_B))\n",
    "data_B_test = pd.read_csv(\"{}/data_test_B.csv\".format(DIR_DATA_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "data_A_train['RESPONSE'] = data_A_train['RESPONSE'].apply(lambda x: preprocess(x))\n",
    "data_A_dev['RESPONSE'] = data_A_dev['RESPONSE'].apply(lambda x: preprocess(x))\n",
    "data_A_test['RESPONSE'] = data_A_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\n",
    "\n",
    "data_B_train['RESPONSE'] = data_B_train['RESPONSE'].apply(lambda x: preprocess(x))\n",
    "data_B_dev['RESPONSE'] = data_B_dev['RESPONSE'].apply(lambda x: preprocess(x))\n",
    "data_B_test['RESPONSE'] = data_B_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\n",
    "\n",
    "stimulus_a = [\"Pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi.\",\n",
    "\"Salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut.\",\n",
    "\"kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan.\",\n",
    "\"Film “Waterworld”, adalah film fiksi ilmiah yang menunjukkan akibat adanya pemanasan global yang sangat besar sehingga menyebabkan bumi menjadi tertutup oleh lautan.\",\n",
    "\"Negara-negara dan daratan yang dulunya kering menjadi tengelamn karena terjadi kenaikan permukaan air laut.\",\n",
    "\"Penduduk yang dulunya bisa berkehidupan bebas menjadi terpaksa mengungsi ke daratan yang lebih tinggi atau tinggal diatas air.\",\n",
    "\"Apa yang akan menjadi tantangan bagi suatu penduduk ketika terjadi situasi daratan tidak dapat ditinggali kembali karena tengelam oleh naiknya air laut.\"]\n",
    "\n",
    "stimulus_b = [\"Sebuah toko baju berkonsep self-service menawarkan promosi dua buah baju bertema tahun baru seharga Rp50.000,00. sebelum baju bertema tahun baru dibagikan kepada pembeli, sebuah layar akan menampilkan tampilan gambar yang menampilkan kondisi kerja di dalam sebuah pabrik konveksi/pembuatan baju. \",\n",
    "\"Kemudian pembeli diberi program pilihan untuk menyelesaikan pembeliannya atau menyumpangkan Rp50.000,00 untuk dijadikan donasi pembagian baju musim dingin di suatu daerah yang membutuhkan.\",\n",
    "\"Delapan dari sepuluh pembeli memilih untuk memberikan donasi.\",\n",
    "\"Menurut anda mengapa banyak dari pembeli yang memilih berdonasi?\"]\n",
    "\n",
    "data_stimulus = []\n",
    "\n",
    "for text in stimulus_a:\n",
    "    data_stimulus.append(preprocess(text))\n",
    "    \n",
    "for text in stimulus_b:\n",
    "    data_stimulus.append(preprocess(text))\n",
    "    \n",
    "data_stimulus.extend(data_A_train['RESPONSE'].values)\n",
    "data_stimulus.extend(data_A_dev['RESPONSE'].values)\n",
    "data_stimulus.extend(data_A_test['RESPONSE'].values)\n",
    "data_stimulus.extend(data_B_train['RESPONSE'].values)\n",
    "data_stimulus.extend(data_B_dev['RESPONSE'].values)\n",
    "data_stimulus.extend(data_B_test['RESPONSE'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi',\n",
       " 'salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut',\n",
       " 'kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data_stimulus))\n",
    "data_stimulus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Open Subtitle Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2019-10-06 22:58:45--  http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.id.gz\n",
      "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
      "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/id.txt.gz [following]\n",
      "--2019-10-06 22:58:46--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/id.txt.gz\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 227979172 (217M) [application/gzip]\n",
      "Saving to: ‘dataset.txt.gz’\n",
      "\n",
      "dataset.txt.gz      100%[===================>] 217.42M  15.1MB/s    in 15s     \n",
      "\n",
      "2019-10-06 22:59:03 (14.5 MB/s) - ‘dataset.txt.gz’ saved [227979172/227979172]\n",
      "\n",
      "Jadiberakhirpetualanganpertamasaya denganMrSherlockHolmes.\n",
      "Ketikasayamelihatkeretanyamenghilang, akumenyadaribahwaaku lupauntukberterimakasihpadanya .\n",
      "Diamengambillemah,anaklaki-lakitakut danmembuatnyamenjadipriayang berani.\n",
      "Hatikumelonjak.\n",
      "Akusudahsiap untuk apapunmisteriataubahayadidepan .\n",
      "Akusudahsiapuntukmengambil petualanganterbesardarimerekasemua.\n",
      "Danakutahuitu pasti untukmelibatkanSherlockHolmes.\n",
      "- Ada yang bisa dibantu?\n",
      "- Saya ingin sebuah ruangan, silakan.\n",
      "Silakan, Pak, untuk menandatangani di sini.\n"
     ]
    }
   ],
   "source": [
    "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.id.gz -O dataset.txt.gz\n",
    "!gzip -d dataset.txt.gz\n",
    "!tail dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Wikipedia Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(link, file_name):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        print(\"Downloading %s\" % file_name)\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in response.iter_content(chunk_size=4096):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50 * dl / total_length)\n",
    "                sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )\n",
    "                sys.stdout.flush()\n",
    "\n",
    "def get_id_wiki(dump_path):\n",
    "    if not os.path.isfile(dump_path):\n",
    "        url = 'https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bz2'\n",
    "        download(url, dump_path)\n",
    "    return gensim.corpora.WikiCorpus(dump_path, lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading idwiki-latest-pages-articles.xml.bz2\n",
      "[==                                                ]"
     ]
    }
   ],
   "source": [
    "dump_path = 'idwiki-latest-pages-articles.xml.bz2'\n",
    "id_wiki = get_id_wiki(dump_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "model_path = 'idwiki_word2vec_{}.model'.format(dim)\n",
    "extracted_path = 'idwiki.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text...\n",
      "total ukara text:  2872\n",
      "total ukara word: 36930\n",
      "total opensubs text:  22760729\n",
      "total opensubs word: 105348108\n",
      "total wikipedia text:  375068\n",
      "total wikipedia word: 101251643\n"
     ]
    }
   ],
   "source": [
    "print('Extracting text...')\n",
    "with open(extracted_path, 'w') as f:\n",
    "    # ukara\n",
    "    i_ukara = 0\n",
    "    word_ukara = 0\n",
    "    for text in data_stimulus:\n",
    "        test_processed = text.strip()\n",
    "        f.write(test_processed + '\\n')\n",
    "        i_ukara += 1\n",
    "        word_ukara += len(test_processed.split())\n",
    "\n",
    "    # opensubs\n",
    "    i_opensubs = 0\n",
    "    word_opensubs = 0\n",
    "    with open('dataset.txt') as f_opensubs:\n",
    "        opensubs = f_opensubs.readlines()\n",
    "        for text in opensubs:\n",
    "            test_processed = preprocess(text).strip()\n",
    "            f.write(test_processed + '\\n')\n",
    "            i_opensubs += 1\n",
    "            word_opensubs += len(test_processed.split())\n",
    "\n",
    "    # wikipedia\n",
    "    i_wiki = 0\n",
    "    word_wiki = 0\n",
    "    for text in id_wiki.get_texts():\n",
    "        text = ' '.join(text)\n",
    "        f.write(text + '\\n')\n",
    "        i_wiki += 1\n",
    "        word_wiki += len(text.split())\n",
    "            \n",
    "    print('total ukara text: ', str(i_ukara))\n",
    "    print('total ukara word:', str(word_ukara))\n",
    "    print('total opensubs text: ', str(i_opensubs))\n",
    "    print('total opensubs word:', str(word_opensubs)) \n",
    "    print('total wikipedia text: ', str(i_wiki))\n",
    "    print('total wikipedia word:', str(word_wiki)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(extracted_path, model_path, dim):\n",
    "    sentences = gensim.models.word2vec.LineSentence(extracted_path)\n",
    "    id_w2v = gensim.models.word2vec.Word2Vec(sentences, size=dim, workers=multiprocessing.cpu_count()-1)\n",
    "    id_w2v.save(model_path)\n",
    "    return id_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Saved model: idwiki_word2vec_100.model\n"
     ]
    }
   ],
   "source": [
    "print('Building the model...')\n",
    "model = build_model(extracted_path, model_path, dim)\n",
    "print('Saved model:', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word2vec vocabulary:  420024\n"
     ]
    }
   ],
   "source": [
    "print(\"Total word2vec vocabulary: \", len(model.wv.vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
