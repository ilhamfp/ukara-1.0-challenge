%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Indonesian Essay Scoring using Bi-LSTM with Word Embedding Representation}

\author{Ilham Firdausi Putra \\
  Sekolah Teknik Elektro dan Informatika \\
  Institut Teknologi Bandung \\
  Bandung, Indonesia \\
  \texttt{ilhamfputra31@gmail.com} \\}


\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper presents the solution that placed 2nd at UKARA 1.0 Challenge 2019. UKARA 1.0 Challenge is an Indonesian automatic essay/short-answer scoring competition held by Universitas Gadjah Mada. We combine Bi-LSTM with pretrained word embedding vector to achieve F1-score of 0.81. The code and pretrained Word2vec word embedding will be made publicly available\footnote{\url{https://github.com/ilhamfp/ukara-1.0-challenge}}.
\end{abstract}

\section{Introduction}

Automatic essay scoring as one of the topics in natural language processing has been greatly developed by the demand to make the assessment process faster. Despite the fast advancement in automatic essay scoring, research in this area for Bahasa Indonesia has been very limited and only recently emerged as a topic. The use of informal language and the diversity of local languages was the main challenge in developing automatic essay scoring for Bahasa Indonesia.

UKARA 1.0 Challenge aims to encourage more ideas and studies for developing automatic short-answer scoring specifically for Bahasa Indonesia. In this challenge, participants will be given access to datasets in the form of students short-answers in two phase. In the first phase, the participant developed their solution with the development set for 43 days from July 29 - September 10, 2019. Finally, the participant can submit their final solution on the second phase with the test set for 3 days from September 16 - September 19, 2019.

\section{Indonesian Essay Scoring}

In this solution, we cast the challenge as a binary classification problem. Given a short-answer to the stimulus, we build a model that tries to predict whether the answer was relevant to the stimulus or not. We process the inputs as a sequence of word. Each word represented as a low dimensional vector and processed sequentially by bidirectional LSTM~\cite{LSTM}.

\subsection{Dataset}

The dataset is a short-answer from 2 different stimuli (For the size detail, see Table 1). The short-answer and stimulus consist of a total 36,930 word with 2,816 unique vocabularies. The label for each short-answer was a binary with 1 representing relevant answer and 0 representing non-relevant answer. The only text preprocessing done was converting character to lowercase and removing non-alphanumeric character.

\begin{table}
\centering
\small
\begin{tabular}{|l|l|l|}
\hline
\textbf{Type of Set} & \textbf{Data A} & \textbf{Data B}\\\hline
{Training} & {268} & {305} \\
{Development} & {215} & {244}  \\ 
{Test} & {855} & {974}  \\\hline
\end{tabular} 
\caption{The total of short-answer for each set type and data type.}
\end{table}

\subsection{Word Embedding}

We pretrained Word2vec~\cite{Word2vec} 100 dimension word embedding using Gensim~\cite{rehurek_lrec} on Indonesian text from Wikipedia dump\footnote{\url{https://dumps.wikimedia.org}}, Opensubs~\cite{opensubs}, and the preprocessed UKARA dataset (For the word count detail, see Table 2). The addition of text from Opensubs and UKARA dataset helps in providing informal words that usually absent in Wikipedia article. With this dataset, we ended up with a total of 420,024 unique vocabularies.

\begin{table}
\centering
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Data Source} & \textbf{Word Count}\\\hline
{Opensubs} & {105348108}\\ 
{Wikipedia} & {101251643}\\
{Ukara} & {36930}\\ 
{Total} & {206636681}\\\hline
\end{tabular} 
\caption{The count of word for each data source.}
\end{table}

\subsection{Model}

We use Keras~\cite{Keras} with Tensorflow~\cite{Tensorflow} as the backend to build the model. The text was tokenized and padded into maximum length of 43 (90th percentile of all short-answer length) before goes into the model. In order to build the embedding layer, we perform a multi-stage text processing using PySastrawi\footnote{\url{https://github.com/har07/PySastrawi}} stemmer and a normalizer function (removing duplicate adjacent character) to minimize the amount of unknown vocabulary (For the count of known word found in each stage, see Table 3). This multi-stage process yields a total of 2.426 known vocabularies and 390 unknown vocabularies. We finally fit the model with an EarlyStopping and ReduceLROnPlateau callback.

\begin{table}
\centering
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Stage} & \textbf{Known Word Count}\\\hline
{1: Raw Word} & {2310}\\ 
{2: Stemmed} & {65}\\
{3: Normalized} & {48}\\ 
{4: Stemmed} & {3}\\\hline
\end{tabular} 
\caption{The count of known word found in each stage of building the word embedding layer.}
\end{table}

\begin{table}
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Type of Data} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall}\\\hline
{Data A CV} & {0.89277} & {0.85238} & {0.93717}\\ 
{Data B CV} & {0.77083} & {0.68519} & {0.88095}\\
{Data Test} & {0.81} & {0.75} & {0.89}\\ \hline
\end{tabular} 
\caption{The cross-validation and final test result.}
\end{table}

\subsection{Experiment}

We run the experiment on RepeatedStratifiedKFold with 10 split and 10 repeats. For each split and repeat, we perform prediction to the validation and test set. We later normalize the result according to how many predictions made, essentially performing ensemble of 100 different model fit. We choose a threshold of 0.5 for Data A and 0.48 for Data B in predicting the label. See Table 4 for the result.

\section{Conclusion}

In this work, we present the effectiveness of Bi-LSTM and pretrained Word2vec in Indonesian essay scoring problem. In addition to that, the inclusion of Opensubs data helps in providing informal words that usually absent in Wikipedia article. Finally, we maximize the amount of known word in building word embedding layer by performing multi-stage text processing.

\bibliography{acl2019}
\bibliographystyle{acl_natbib}

\appendix

\section{Hyperparameter Detail}
\subsection{Gensim Hyperparameter}
We use \verb|gensim.models.word2vec.Word2Vec| default parameter as of version 3.8.1.
\subsection{Model Hyperparameter}
We use the default parameter as of Keras version 2.3.0 and Tensorflow version 1.14.0 as the backend with the exception of the following:
\\

\textbf{Bi-LSTM}:
\begin{itemize}
\item \verb|units|: 50
\item \verb|return_sequences|: True
\item \verb|return_dropout|: 0.1
\item \verb|return_recurrent_dropout|: 0.1
\end{itemize}

\textbf{Dropout}:
\begin{itemize}
\item \verb|rate|: 0.1
\end{itemize}

\textbf{EarlyStopping}:
\begin{itemize}
\item \verb|monitor|: 'val\_f1'
\item \verb|min_delta|: 0.0001
\item \verb|patience|: 8
\item \verb|mode|: 'max'
\item \verb|baseline|: None
\item \verb|restore_best_weights|: True
\end{itemize}

\textbf{ReduceLROnPlateau}:
\begin{itemize}
\item \verb|monitor|: 'val\_f1'
\item \verb|factor|: 0.5
\item \verb|patience|: 3
\item \verb|mode|: 'max'
\item \verb|min_lr|: 1e-6
\end{itemize}

\end{document}
